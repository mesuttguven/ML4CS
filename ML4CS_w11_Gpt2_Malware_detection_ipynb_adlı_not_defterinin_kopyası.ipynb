{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesuttguven/ML4CS/blob/main/ML4CS_w11_Gpt2_Malware_detection_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPECIAL NOTE:**\n",
        "\n",
        "This Colab belongs to *Nazenin Sahin (M.S. in Cyber Security)* who is a distinguished colleague of mine. \n",
        "\n",
        "The model was created during her dissertation thesis and M.S. studies."
      ],
      "metadata": {
        "id": "C4Rz4NoCtzcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCYe0kk1iMxr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5O2GCfyi7j7"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "# Install helper functions.\n",
        "!pip install -q git+https://github.com/gmihaila/ml_things.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt48NtLna8Js"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.7.0 #set_seed error and model = RobertaForSequenceClassification row error\n",
        "!pip install transformers==3.5.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjr_J342tOPq"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (set_seed,\n",
        "                          TrainingArguments,\n",
        "                          Trainer,\n",
        "                          GPT2Config,\n",
        "                          GPT2Tokenizer,\n",
        "                          AdamW, \n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          GPT2ForSequenceClassification)\n",
        "\n",
        "# Set seed for reproducibility.\n",
        "set_seed(82)\n",
        "\n",
        "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
        "epochs = 2\n",
        "\n",
        "# Number of batches - depending on the max sequence length and GPU memory.\n",
        "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
        "# For small sequence length can try batch of 32 or higher.\n",
        "batch_size = 64\n",
        "\n",
        "# Pad or truncate text sequences to a specific length\n",
        "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
        "max_length = 45\n",
        "\n",
        "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Name of transformers model - will use already pretrained model.\n",
        "# Path of transformer model - will load your own model from local disk.\n",
        "#model_name_or_path = './drive/My Drive/TEZ/GPT2/GPT2_3000000/GPT2_3000000_byte_model'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoJYfRwKuTOc"
      },
      "outputs": [],
      "source": [
        "!wc -l './drive/My Drive/own_data/malicious.txt'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvnZ9NEmvUuq"
      },
      "outputs": [],
      "source": [
        "!head -n 5000000 './drive/My Drive/own_data/benign.txt' > './drive/My Drive/data_5/benign.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3USnkW9i7oQ"
      },
      "outputs": [],
      "source": [
        "class MalBengDataset(Dataset):\n",
        "  r\"\"\"PyTorch Dataset class for loading data.\n",
        "\n",
        "  This is where the data parsing happens.\n",
        "\n",
        "  This class is built with reusability in mind: it can be used as is as.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    path (:obj:`str`):\n",
        "        Path to the data partition.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, path, use_tokenizer):\n",
        "\n",
        "    self.texts = []\n",
        "    self.labels = []\n",
        "\n",
        "    \n",
        "\n",
        "    DATA_DIR=path\n",
        "    MERGED_MAL_FILE=os.path.join(DATA_DIR,'malicious.txt')\n",
        "    print(MERGED_MAL_FILE)\n",
        "    MERGED_BENIGN_FILE=os.path.join(DATA_DIR,'benign.txt')\n",
        "    print(MERGED_BENIGN_FILE)\n",
        "\n",
        "    FILE_NAMES = [file for file in os.listdir(DATA_DIR)]\n",
        "    print(FILE_NAMES)\n",
        "    for i, file_name in enumerate(FILE_NAMES):\n",
        "        with open(os.path.join(DATA_DIR,file_name), \"r\") as f:\n",
        "            for line in f:\n",
        "                self.texts.append(line)\n",
        "                self.labels.append(i)\n",
        "        print(i)\n",
        "        print(file_name)\n",
        "\n",
        "    # Number of exmaples.\n",
        "    self.n_examples = len(self.labels)\n",
        "    \n",
        "\n",
        "    return\n",
        "\n",
        "  def __len__(self):\n",
        "    r\"\"\"When used `len` return the number of examples.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return self.n_examples\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    r\"\"\"Given an index return an example from the position.\n",
        "    \n",
        "    Arguments:\n",
        "\n",
        "      item (:obj:`int`):\n",
        "          Index position to pick an example to return.\n",
        "\n",
        "    Returns:\n",
        "      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
        "      asociated labels.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return {'text':self.texts[item],\n",
        "            'label':self.labels[item]}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNZ-eKHoi7rD"
      },
      "outputs": [],
      "source": [
        "class GPT2ClassificationCollator(object):\n",
        "    r\"\"\"\n",
        "    Data Collator used for GPT2 in a classificaiton rask. \n",
        "    \n",
        "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
        "    can go straight into a GPT2 model.\n",
        "\n",
        "    This class is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
        "          Transformer type tokenizer used to process raw text into numbers.\n",
        "\n",
        "      labels_ids (:obj:`dict`):\n",
        "          Dictionary to encode any labels names into numbers. Keys map to \n",
        "          labels names and Values map to number associated to those labels.\n",
        "\n",
        "      max_sequence_len (:obj:`int`, `optional`)\n",
        "          Value to indicate the maximum desired sequence to truncate or pad text\n",
        "          sequences. If no value is passed it will used maximum sequence size\n",
        "          supported by the tokenizer and model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_tokenizer, max_sequence_len=None):\n",
        "\n",
        "        # Tokenizer to be used inside the class.\n",
        "        self.use_tokenizer = use_tokenizer\n",
        "        # Check max sequence length.\n",
        "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        r\"\"\"\n",
        "        This function allowes the class objesct to be used as a function call.\n",
        "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
        "        class as a function.\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "          item (:obj:`list`):\n",
        "              List of texts and labels.\n",
        "\n",
        "        Returns:\n",
        "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
        "          It holddes the statement `model(**Returned Dictionary)`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get all texts from sequences list.\n",
        "        texts = [sequence['text'] for sequence in sequences]\n",
        "        # Get all labels from sequences list.\n",
        "        labels = [sequence['label'] for sequence in sequences]\n",
        "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
        "        # appropriate padding.\n",
        "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "        # Update the inputs with the associated encoded labels as tensor.\n",
        "        inputs.update({'labels':torch.tensor(labels)})\n",
        "\n",
        "        #print(inputs)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def train(dataloader, optimizer_, scheduler_, device_):\n",
        "\n",
        "  # Use global variable for model.\n",
        "  global model\n",
        "\n",
        "  # Tracking variables.\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  # Total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Put the model into training mode.\n",
        "  model.train()\n",
        "\n",
        "  # For each batch of training data...\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    # Add original labels - use later for evaluation.\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "    \n",
        "    # move batch to device\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "    \n",
        "    # Always clear any previously calculated gradients before performing a\n",
        "    # backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Perform a forward pass (evaluate the model on this training batch).\n",
        "    # This will return the loss (rather than the model output) because we\n",
        "    # have provided the `labels`.\n",
        "    # The documentation for this a bert model function is here: \n",
        "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    # The call to `model` always returns a tuple, so we need to pull the \n",
        "    # loss value out of the tuple along with the logits. We will use logits\n",
        "    # later to calculate training accuracy.\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # Accumulate the training loss over all of the batches so that we can\n",
        "    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "    # single value; the `.item()` function just returns the Python value \n",
        "    # from the tensor.\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Perform a backward pass to calculate the gradients.\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
        "\n",
        "    # Update parameters and take a step using the computed gradient.\n",
        "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "    # modified based on their gradients, the learning rate, etc.\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the learning rate.\n",
        "    scheduler.step()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Convert these logits to list of predicted labels values.\n",
        "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "  \n",
        "  # Return all true labels and prediction for future evaluations.\n",
        "  return true_labels, predictions_labels, avg_epoch_loss\n",
        "\n",
        "\n",
        "\n",
        "def validation(dataloader, device_):\n",
        "  r\"\"\"Validation function to evaluate model performance on a \n",
        "  separate set of data.\n",
        "\n",
        "  This function will return the true and predicted labels so we can use later\n",
        "  to evaluate the model's performance.\n",
        "\n",
        "  This function is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
        "          Parsed data into batches of tensors.\n",
        "\n",
        "    device_ (:obj:`torch.device`):\n",
        "          Device used to load tensors before feeding to model.\n",
        "\n",
        "  Returns:\n",
        "    \n",
        "    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
        "        Labels, Train Average Loss]\n",
        "  \"\"\"\n",
        "\n",
        "  # Use global variable for model.\n",
        "  global model\n",
        "\n",
        "  # Tracking variables\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  #total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    # add original labels\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "\n",
        "    # move batch to device\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up validation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have\n",
        "        # not provided labels.\n",
        "        # token_type_ids is the same as the \"segment ids\", which \n",
        "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple along with the logits. We will use logits\n",
        "        # later to to calculate training accuracy.\n",
        "        loss, logits = outputs[:2]\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # get predicitons to list\n",
        "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "        # update list\n",
        "        predictions_labels += predict_content\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "\n",
        "  # Return all true labels and prediciton for future evaluations.\n",
        "  return true_labels, predictions_labels, avg_epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzQdPqVVff_w"
      },
      "outputs": [],
      "source": [
        "# Get model configuration.\n",
        "print('Loading configuraiton...')\n",
        "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path='./drive/My Drive/GPT2/Gpt2_son_gpt/Gpt2_son_model/gpt2_28_06/checkpoint-600000', num_labels=2)\n",
        "#model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path='EleutherAI/gpt-neo-1.3B', num_labels=2)\n",
        "\n",
        "\n",
        "#model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path='gpt2-medium', num_labels=2)\n",
        "# Get model's tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='EleutherAI/gpt-neo-1.3B')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='./drive/My Drive/GPT2/Gpt2_son_gpt/Gpt2_son_config')\n",
        "special_tokens_dict = {\"bos_token\": '<bos>', \"eos_token\": '<eos>', \"unk_token\": '<unk>', \"sep_token\": '<sep>',\n",
        "                                 \"pad_token\":'<pad>', \"cls_token\": '<cls>', \"mask_token\" : '<mask>'}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "\n",
        "# default to left padding\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Define PAD Token = EOS Token = 50256\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Get the actual model.\n",
        "print('Loading model...')\n",
        "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path='./drive/My Drive/GPT2/Gpt2_son_gpt/Gpt2_son_model/gpt2_28_06/checkpoint-600000', config=model_config)\n",
        "#model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path='EleutherAI/gpt-neo-1.3B', config=model_config)\n",
        "\n",
        "# resize model embedding to match new tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# fix model padding token id\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Load model to defined device.\n",
        "model.to(device)\n",
        "print('Model loaded to `%s`'%device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKHd6rnqPrgt"
      },
      "outputs": [],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnovYLHOh0Ha"
      },
      "outputs": [],
      "source": [
        "#tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqn63aWEh0aP"
      },
      "outputs": [],
      "source": [
        "#special_tokens_dict = {'pad_token': '<PAD>'}\n",
        "#num_added_toks = tokenizer.add_special_tokens(SPECIAL_TOKENS_ATTRIBUTES)\n",
        "#model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAY_IMviixe_"
      },
      "outputs": [],
      "source": [
        "#model.config.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO_Np99yhvEr"
      },
      "outputs": [],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUHu6UKfRmdL"
      },
      "outputs": [],
      "source": [
        "tokenizer.all_special_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deENkG2ZRwZX"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"sub eax,DWORD PTR [eax]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGazsuT2R24L"
      },
      "outputs": [],
      "source": [
        "sent=\"sub eax,DWORD PTR [eax]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmL05OcPTJMX"
      },
      "outputs": [],
      "source": [
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 45,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxoY8NvvYACV"
      },
      "outputs": [],
      "source": [
        "encoded_dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wEJd_IYoqV"
      },
      "outputs": [],
      "source": [
        "encoded_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiyY9t2efonC"
      },
      "outputs": [],
      "source": [
        "#tokenizer.sep_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyiK--1P_gGy"
      },
      "outputs": [],
      "source": [
        "#model.cuda()>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpjqnQmEd6eq"
      },
      "outputs": [],
      "source": [
        "\n",
        "#from sklearn import datasets, linear_model\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcKdIpWnehjH"
      },
      "outputs": [],
      "source": [
        "GPT2_classificaiton_collator = GPT2ClassificationCollator(use_tokenizer=tokenizer, max_sequence_len=max_length)\n",
        "\n",
        "\n",
        "print('Dealing with Train...')\n",
        "# Create pytorch dataset.\n",
        "gpt2_dataset = MalBengDataset(path='./drive/My Drive/data_6', use_tokenizer=tokenizer)\n",
        "print('Created `all_dataset` with %d examples!'%len(gpt2_dataset))\n",
        "\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
        "\n",
        "total_count=len(gpt2_dataset) #splt data for train,validation and train\n",
        "train_count = int(0.8 * total_count)\n",
        "print('Created `train_dataset` with %d examples!'%(train_count))\n",
        "valid_count = int(0.2 * total_count)\n",
        "print('Created `valid_count` with %d examples!'%(valid_count))\n",
        "\n",
        "train_dataset, valid_dataset= torch.utils.data.random_split(\n",
        "    gpt2_dataset, (train_count, valid_count))#+1 ekledim neden?anlamadım\n",
        "\n",
        "\n",
        "# Move pytorch dataset into dataloader.\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=GPT2_classificaiton_collator)\n",
        "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
        "\n",
        "\n",
        "print('Dealing with Validation...')\n",
        "# Create pytorch dataset.\n",
        "#valid_dataset =  MalBengDataset(path='./drive/My Drive/TEZ/test', \n",
        "                               #use_tokenizer=tokenizer)\n",
        "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
        "\n",
        "# Move pytorch dataset into dataloader.\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=GPT2_classificaiton_collator)\n",
        "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maXoUzuzgF7l"
      },
      "outputs": [],
      "source": [
        "max_len_gpt2 = 0\n",
        "\n",
        "\n",
        "# For every data_sets element...\n",
        "for sent in data_set:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids_gpt2 = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum data_set length.\n",
        "    max_len_gpt2 = max(max_len_gpt2, len(input_ids_gpt2))\n",
        "    \n",
        "print('Max sentence length GPT2: ', max_len_gpt2)\n",
        "#Max sentence length gpt2:  34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLPCkiv-lis3"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
        "# us the number of batches.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "all_loss = {'train_loss':[], 'val_loss':[]}\n",
        "all_acc = {'train_acc':[], 'val_acc':[]}\n",
        "\n",
        "# Loop through each epoch.\n",
        "print('Epoch')\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print()\n",
        "  print('Training on batches...')\n",
        "  # Perform one full pass over the training set.\n",
        "  train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
        "  train_acc = accuracy_score(train_labels, train_predict)\n",
        "\n",
        "  # Get prediction form model on validation data. \n",
        "  print('Validation on batches...')\n",
        "  valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
        "  val_acc = accuracy_score(valid_labels, valid_predict)\n",
        "\n",
        "  # Print loss and accuracy values to see how training evolves.\n",
        "  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
        "  print()\n",
        "\n",
        "  # Store the loss value for plotting the learning curve.\n",
        "  all_loss['train_loss'].append(train_loss)\n",
        "  all_loss['val_loss'].append(val_loss)\n",
        "  all_acc['train_acc'].append(train_acc)\n",
        "  all_acc['val_acc'].append(val_acc)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyn5vRKFV7Wg"
      },
      "outputs": [],
      "source": [
        "12 milyon data,batch_size=64,3 epochs,clip_grad=1,0 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2kqfYO0EG1J"
      },
      "outputs": [],
      "source": [
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap1DjuQTEfcI"
      },
      "outputs": [],
      "source": [
        "labels_ids = {'neg': 0, 'pos': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DufW3SeIFBO0"
      },
      "outputs": [],
      "source": [
        "!pip uninstall matplotlib\n",
        "!pip install matplotlib==3.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFoBZ82oBavT"
      },
      "outputs": [],
      "source": [
        "# Get prediction form model on validation data. This is where you should use\n",
        "# your test data.\n",
        "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
        "\n",
        "# Create the evaluation report.\n",
        "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
        "# Show the evaluation report.\n",
        "print(evaluation_report)\n",
        "\n",
        "# Plot confusion matrix.\n",
        "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, \n",
        "                      classes=list(labels_ids.keys()), normalize=True, \n",
        "                      magnify=0.1,\n",
        "                      );"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xomVbn8ShWYX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVKLlOTQhXDZ"
      },
      "outputs": [],
      "source": [
        "sklearn.metrics.matthews_corrcoef(true_labels, predictions_labels, sample_weight=None )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML4CS_w11_Gpt2_Malware_detection.ipynb adlı not defterinin kopyası",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}